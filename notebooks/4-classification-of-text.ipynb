{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Features from Text\n",
    "\n",
    "![](images/textf1.png)<br>\n",
    "![](images/textf2.png)<br>\n",
    "![](images/textf3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "## Case study: Classifying text search queries\n",
    "* Suppose you are interested in classifying search queries in three classes - entertainment, computer science, zoology\n",
    "* Most common class of the three is Entertainment\n",
    "* Suppose the query is **\"Python\"**\n",
    "    * Python , the snake (zoology)\n",
    "    * Python, the programming language (computer science)\n",
    "    * Python, as in Monty Python (Entertainment) \n",
    "* Most common class, given \"python\", is Zoology.\n",
    "* Suppose the query is **\"Python download\"**\n",
    "    * Most probable class if Computer Science\n",
    "* Update the liklihood of the class fiven new information\n",
    "* Prior Probabity: Pr(y=Entertainment), pr(y=CS), pr(y=zoology)\n",
    "* Posterior probability: Pr(y=Entertainment|x=\"python\")\n",
    "\n",
    "## Bayes' Rule\n",
    "* $ Posterior  probability = \\frac{Prior probability * Likelihood}{Evidence} $\n",
    "\n",
    "* $ Pr(y|X) = \\frac{Pr(y) * Pr(X|y)}{Pr(X)} $\n",
    "\n",
    "### Naïve Bayes Classification\n",
    "\n",
    "* $Pr(y=CS|\"Python\") = \\frac{Pr(y=CS) *Pr(\"Python\"|y=CS)}{Pr(\"Python\")}$\n",
    "* $Pr(y=zoology|\"Python\") = \\frac{Pr(y=zoology) *Pr(\"Python\"|y=zoology)}{Pr(\"Python\")}$\n",
    "* $Pr(y=CS|\"Python\")>Pr(y=zoology|\"Python\") $ then y=CS\n",
    "\n",
    "\n",
    "\n",
    "so...\n",
    "\n",
    "$ y^* = argmax(y)Pr(y|X) =  argmax(y)Pr(y)  \\times Pr(X|y)$\n",
    "\n",
    "* **Naive assumption**: Given the class label, features are assumed to be independent of each other\n",
    "\n",
    "$ y^* = {argmax(y)}{Pr(y|X)} =  {argmax(y)} {Pr(y)} \\times{\\displaystyle\\prod_{i=1}^{n} Pr(x_i|y)}$\n",
    "\n",
    "if query is <span style=\"color:blue\"> **\"Python download\"** </span>\n",
    "\n",
    "$ y^* ={argmax(y)} {Pr(\"Python\"|X)} = {argmax(y)} {Pr(y)} \\times{Pr(\"download\"|y)}$\n",
    "\n",
    "### Nïve Bayes: Parameters\n",
    "\n",
    "* **Prior probabilities:** $Pr(y)$ for all $y$ in $Y$\n",
    "* **Likelihood:** $Pr(X_i|y)$ for all features $x_i$ and labels $y$ in $Y$\n",
    "\n",
    "* **Example**\n",
    "If tehre are 3 classes $(|Y|=3)$ and 100 features in $X$, how many parameters does naïve Bayes models have?\n",
    "\n",
    "$$|Y| + 2 x |X| +x|Y| = 603$$\n",
    "\n",
    "A naïve Bayes classifier has two kinds of parameters:\n",
    "$Pr(y)$ for every $y in Y$: so if $|Y| = 3$, there are three such parameters.\n",
    "$Pr(x_i | y)$ for every binary feature $x_i$ in $X$ and $y$ in $Y$. Specifically, for a particular feature $x_1$, the parameters are $Pr(x_1 = 1 | y)$ and $Pr(x_1 = 0 | y)$ for every $y$. So if $|X| = 100$ binary features and $|Y| = 3$, there are $(2 x 100) x 3 = 600$ such features\n",
    "\n",
    "### Naïve Bayes: Learning Parameters\n",
    "\n",
    "* **Prior probabilities:** $Pr(y)$ for all $y$ in $Y$\n",
    "    * Training data\n",
    "    * Count the number of instances in each class\n",
    "    * If there are $N$ instances in al, and $n$ out of those are labeled as class $y$ -> $Pr(y)= \\frac{n}{N}\n",
    "\n",
    "* **Likelihood:** $Pr(X_i|y)$ for all features $x_i$ and labels $y$ in $Y$\n",
    "    * Count how many times feature $x_i$ appears in instances labeled as class $y$\n",
    "\n",
    "* **Smoothing**\n",
    "    * What hapens if $Pr(x_i|y) =0$? In case when you have never seen a word\n",
    "        * Features $x_i$ never occurs in documents labeled $y$\n",
    "        * But then, the posterior probability $Pr(y|x_i)$ will be 0\n",
    "    * Instead, smooth the parameters\n",
    "    * **Laplace smoothing** or **Additive smoothing**: add a dummy count\n",
    "        * $Pr(x_i|y)= \\frac{k+1}{p+n}$; where n is number of features\n",
    "        \n",
    "        \n",
    "### Summary\n",
    "*  Naïve Bayes is a probabilistic model\n",
    "*  Naïve, because is assumes features are independent of each other, fiven the class label -- not true in case of White House are they are not independent - Naïve Bayes ignores\n",
    "*  For text classificaiton problems, Naïve Bayes models typically provide very strong baselines\n",
    "*  Simple model, easy\n",
    "\n",
    "## Naïve Bayes variations\n",
    "\n",
    "### Two Classic Naïve Bayes Variants for Text\n",
    "\n",
    "* There are two ways in which Naïve Bayes features could be learned\n",
    "* Multinomial Naïve Bayes model\n",
    "    * Data follows a multinomial distribution\n",
    "    * Each feature value is a count (word occurence counts, TF-IDF weighting, ...)\n",
    "    \n",
    "     Suppose you have a piece of text, a document, and you are finding out what are all the words that were used in this model. That would be called a bag-of-words model. And if you just use the words, whether they were present or not, then that is a Bernoulli distribution for each feature. So, it becomes a multivariate Bernoulli when you're talking about it for all the words. But if you say that the number of times a particular word occurs is important, so for example, if the statement is **to be** or **not to be**, and you want to somehow say that the word to occur twice, the word *OR* occur just once and so on, you want to somehow keep track of what was the frequency of each of these words. And then, if you want to give more importance to more rare words, then you would add on something called a term frequency, inverse document frequency weighting. So you don't, not only give importance to the frequency, but say how common is this word in the entire collection, and that's what the idea of weighting comes from. So for example, the word **THE** is very common, it occurs on almost every sentence, it occurs in every document, so it is not very informative. But if it is the word, like, **SIGNIFICANT**, it is significant because it's not gonna be occurring in every document. So, you want to give a higher importance to a document that has this word significant as compared to the word the, and that kind of variation in weighting is possible when you're doing a multinomial Naïve Bayes model. \n",
    "     \n",
    "     \n",
    "* Bernoulli model\n",
    "    * Data follows a multivariate Bernoulli distribution\n",
    "    * Each feature is binary (word is present/absent) - no matter how many times\n",
    "<hr>    \n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "![](images/svm1.png)<br>\n",
    "![](images/svm2.png)<br>\n",
    "![](images/svm3.png)<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Decision Boundaries\n",
    "![](images/svm4.png)<br>\n",
    "![](images/svm5.png)<br>\n",
    "\n",
    "Linear boundary has better accuracy than the above\n",
    "![](images/svm6.png)<br>\n",
    "![](images/svm7.png)<br>\n",
    "\n",
    "![](images/svm7.png)<br>\n",
    "\n",
    "In linear regression, a small change in data will affect the classifier.\n",
    "![](images/svm8.png)<br>\n",
    "\n",
    "With boundaries, the classifier is more resistant to the noise -> **maximum margin**\n",
    "![](images/svm9.png)<br>\n",
    "\n",
    "SVM are the maximum margin classifier\n",
    "![](images/svm10.png)<br>\n",
    "\n",
    "### SVM: Binary Classification\n",
    "\n",
    "* SVM are **linear classifiers** that find a hyperplane to separate **two classes** of data: positive and negative\n",
    "* Given training data $(X_1, y_1),(X_2, y_2),...;$ where $X_i = (X_1, X_2, ... X_n)$ is instance vector and $y_i$ is one of **{-1, +1}**\n",
    "    * SVM finds a linear function w (weight vector) (Binary classification)\n",
    "    \n",
    "    $ f(X_i) = <w . X_i> = b $\n",
    "    if $f(X_i) >= 0, y_i = +1; else y_i= -1$\n",
    "    \n",
    "### SVM: Multi-class Classification\n",
    "\n",
    "* SVM works only for binary classification problems\n",
    "    $ f(X_i) = <w . X_i> = b $\n",
    "    if $f(X_i) >= 0, y_i = +1; else y_i= -1$\n",
    "    \n",
    "* What about three classes?\n",
    "    * One vs Rest\n",
    "![](images/svm11.png)<br>\n",
    "![](images/svm12.png)<br>\n",
    "![](images/svm13.png)<br>\n",
    "![](images/svm14.png)<br>\n",
    "\n",
    "    * One vs One\n",
    "![](images/svm15.png)<br>    \n",
    "\n",
    "### SVM Parameters (1): Parameter C\n",
    "\n",
    "* Regularization: How much importance should yopu five individual data points as compared to better generalized model = how much importance we give to individual data points\n",
    "If you have a larger value of C, that means the reguylarization is less, and that means that you are fitting the training data as mucch as possible.\n",
    "\n",
    "* Regularization Parameter C\n",
    "    * Larger values of C = Less regularization\n",
    "        * Fit training data as well as possible, every data point important\n",
    "    * Smaller values of C = more regularization\n",
    "        * More tolerant to errors or noise on individual data points\n",
    "\n",
    "### SVM Parameters (2): Other Parameters\n",
    "What is the type of a decision boundary you would want to learn.\n",
    "\n",
    "* Linear kernels usually work best for text data\n",
    "    * Other kernels inculude RBF, Polynomial\n",
    "    * multi_class: ovr (one-vs-rest)\n",
    "    * class_weight: Different classes can get different weights\n",
    "\n",
    "SVM - Summary\n",
    "* SVM tend to be the most accurate classifiers, especially in high - dimensional data\n",
    "* Strong theoretical foundation\n",
    "* Handles only numeric features\n",
    "    * hot encoding\n",
    "    * normalization\n",
    "* Difficult to intepret hyperplane\n",
    "\n",
    "### Quick Recap of RBF\n",
    "This example illustrates the effect of the parameters `gamma` and `C` of the Radial Basis Function (RBF) kernel SVM.\n",
    "\n",
    "Intuitively, the `gamma` parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The `gamma` parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    "The behavior of the model is very sensitive to the `gamma` parameter. If `gamma` is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with C will be able to prevent overfitting.\n",
    "\n",
    "When `gamma` is very small, the model is too constrained and cannot capture the complexity or “shape” of the data. The region of influence of any selected support vector would include the whole training set. The resulting model will behave similarly to a linear model with a set of hyperplanes that separate the centers of high density of any pair of two classes.\n",
    "\n",
    "The `C` parameter trades off correct classification of training examples against maximization of the decision function’s margin. For larger values of `C`, a `smaller margin` will be accepted if the decision function is better at classifying all training points correctly. A lower `C` will encourage a `larger margin`, therefore a simpler decision function, at the cost of training accuracy. In other words``C`` behaves as a regularization parameter in the SVM.\n",
    "\n",
    "The first plot is a visualization of the decision function for a variety of parameter values on a simplified classification problem involving only 2 input features and 2 possible target classes (binary classification). Note that this kind of plot is not possible to do for problems with more features or target classes.\n",
    "\n",
    "![](images/sphx_glr_plot_rbf_parameters_001.png)\n",
    "\n",
    "The second plot is a heatmap of the classifier’s cross-validation accuracy as a function of C and gamma. For this example we explore a relatively large grid for illustration purposes. In practice, a logarithmic grid from  to  is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search.\n",
    "\n",
    "![](images/sphx_glr_plot_rbf_parameters_002.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
